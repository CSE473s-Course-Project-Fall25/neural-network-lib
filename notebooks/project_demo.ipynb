{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE473s Project\n",
    "\n",
    "## Part 1 (XOR)\n",
    "Implement our custom library, test it with [XOR] problem, validate it, and also implement the same exact problem using [TesnorFlow] or [Keras] then compare the results.\n",
    "\n",
    "### Section-1 (Gradient Checking)\n",
    "\n",
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_true = np.array([[0],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build netwrok with the custom library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from lib import Sequential, Dense, Tanh, Sigmoid, MSELoss, SGDOptimizer as SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2, 4, 1.0), Tanh(),\n",
    "    Dense(4, 1, 1.0), Sigmoid()\n",
    "])\n",
    "\n",
    "opt = SGD(learning_rate=1.0)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2469483054359347\n",
      "Epoch 100, Loss: 0.020644006458808983\n",
      "Epoch 200, Loss: 0.006000519651884105\n",
      "Epoch 300, Loss: 0.003312250491557\n",
      "Epoch 400, Loss: 0.0022506851995748233\n",
      "Epoch 500, Loss: 0.0016924667931390344\n",
      "Epoch 600, Loss: 0.0013510226477977515\n",
      "Epoch 700, Loss: 0.0011216587472929263\n",
      "Epoch 800, Loss: 0.0009574344056137714\n",
      "Epoch 900, Loss: 0.0008342844070182186\n",
      "Epoch 1000, Loss: 0.0007386406327711276\n",
      "Epoch 1100, Loss: 0.0006622893073454459\n",
      "Epoch 1200, Loss: 0.0005999752704485083\n",
      "Epoch 1300, Loss: 0.0005481840802363755\n",
      "Epoch 1400, Loss: 0.0005044786339616938\n",
      "Epoch 1500, Loss: 0.00046711700835226557\n",
      "Epoch 1600, Loss: 0.0004348218417794829\n",
      "Epoch 1700, Loss: 0.00040663561088157636\n",
      "Epoch 1800, Loss: 0.0003818267200022188\n",
      "Epoch 1900, Loss: 0.0003598267736524325\n",
      "Epoch 2000, Loss: 0.00034018760471100787\n",
      "Epoch 2100, Loss: 0.000322551172906021\n",
      "Epoch 2200, Loss: 0.00030662805726915435\n",
      "Epoch 2300, Loss: 0.00029218181451757933\n",
      "Epoch 2400, Loss: 0.00027901742087783394\n",
      "Epoch 2500, Loss: 0.0002669726073912922\n",
      "Epoch 2600, Loss: 0.00025591127873591184\n",
      "Epoch 2700, Loss: 0.0002457184544510918\n",
      "Epoch 2800, Loss: 0.00023629633755405555\n",
      "Epoch 2900, Loss: 0.00022756122834772654\n",
      "Epoch 3000, Loss: 0.00021944107906971948\n",
      "Epoch 3100, Loss: 0.0002118735395489253\n",
      "Epoch 3200, Loss: 0.00020480438273389704\n",
      "Epoch 3300, Loss: 0.00019818622677307334\n",
      "Epoch 3400, Loss: 0.00019197749055525768\n",
      "Epoch 3500, Loss: 0.0001861415344897588\n",
      "Epoch 3600, Loss: 0.00018064594934980905\n",
      "Epoch 3700, Loss: 0.00017546196428294743\n",
      "Epoch 3800, Loss: 0.00017056395135554858\n",
      "Epoch 3900, Loss: 0.00016592900877631197\n",
      "Epoch 4000, Loss: 0.00016153660861679525\n",
      "Epoch 4100, Loss: 0.0001573682976922298\n",
      "Epoch 4200, Loss: 0.00015340744248499942\n",
      "Epoch 4300, Loss: 0.00014963901073587887\n",
      "Epoch 4400, Loss: 0.00014604938370506827\n",
      "Epoch 4500, Loss: 0.00014262619419952834\n",
      "Epoch 4600, Loss: 0.00013935818633832697\n",
      "Epoch 4700, Loss: 0.0001362350937309332\n",
      "Epoch 4800, Loss: 0.000133247533311695\n",
      "Epoch 4900, Loss: 0.00013038691253510574\n",
      "Epoch 5000, Loss: 0.00012764534801282048\n",
      "Epoch 5100, Loss: 0.00012501559398176\n",
      "Epoch 5200, Loss: 0.00012249097924638838\n",
      "Epoch 5300, Loss: 0.00012006535144800009\n",
      "Epoch 5400, Loss: 0.00011773302768775001\n",
      "Epoch 5500, Loss: 0.00011548875067506435\n",
      "Epoch 5600, Loss: 0.00011332764969408536\n",
      "Epoch 5700, Loss: 0.0001112452057823799\n",
      "Epoch 5800, Loss: 0.0001092372206015214\n",
      "Epoch 5900, Loss: 0.00010729978855137227\n",
      "Epoch 6000, Loss: 0.00010542927174083964\n",
      "Epoch 6100, Loss: 0.00010362227747988772\n",
      "Epoch 6200, Loss: 0.00010187563800169116\n",
      "Epoch 6300, Loss: 0.00010018639216161982\n",
      "Epoch 6400, Loss: 9.855176889204332e-05\n",
      "Epoch 6500, Loss: 9.696917221972225e-05\n",
      "Epoch 6600, Loss: 9.54361676764794e-05\n",
      "Epoch 6700, Loss: 9.395046995444844e-05\n",
      "Epoch 6800, Loss: 9.25099316750486e-05\n",
      "Epoch 6900, Loss: 9.111253315633363e-05\n",
      "Epoch 7000, Loss: 8.97563730767777e-05\n",
      "Epoch 7100, Loss: 8.843965994530209e-05\n",
      "Epoch 7200, Loss: 8.716070429753617e-05\n",
      "Epoch 7300, Loss: 8.59179115472716e-05\n",
      "Epoch 7400, Loss: 8.470977542986789e-05\n",
      "Epoch 7500, Loss: 8.35348719812743e-05\n",
      "Epoch 7600, Loss: 8.23918540023518e-05\n",
      "Epoch 7700, Loss: 8.127944596348464e-05\n",
      "Epoch 7800, Loss: 8.019643930925448e-05\n",
      "Epoch 7900, Loss: 7.914168812697781e-05\n",
      "Epoch 8000, Loss: 7.81141051467112e-05\n",
      "Epoch 8100, Loss: 7.711265804350317e-05\n",
      "Epoch 8200, Loss: 7.613636601561727e-05\n",
      "Epoch 8300, Loss: 7.518429661500856e-05\n",
      "Epoch 8400, Loss: 7.425556280865186e-05\n",
      "Epoch 8500, Loss: 7.334932025134479e-05\n",
      "Epoch 8600, Loss: 7.246476475247367e-05\n",
      "Epoch 8700, Loss: 7.16011299208402e-05\n",
      "Epoch 8800, Loss: 7.075768497313026e-05\n",
      "Epoch 8900, Loss: 6.993373269293644e-05\n",
      "Epoch 9000, Loss: 6.912860752840153e-05\n",
      "Epoch 9100, Loss: 6.834167381762965e-05\n",
      "Epoch 9200, Loss: 6.757232413199076e-05\n",
      "Epoch 9300, Loss: 6.681997772827752e-05\n",
      "Epoch 9400, Loss: 6.60840791014614e-05\n",
      "Epoch 9500, Loss: 6.53640966305276e-05\n",
      "Epoch 9600, Loss: 6.465952131046515e-05\n",
      "Epoch 9700, Loss: 6.396986556410932e-05\n",
      "Epoch 9800, Loss: 6.329466212801589e-05\n",
      "Epoch 9900, Loss: 6.26334630070625e-05\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "model.fit(X, y_true, loss_fn, opt, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw preds:\n",
      " [[0.00321298]\n",
      " [0.99170442]\n",
      " [0.99166056]\n",
      " [0.00996279]]\n",
      "Rounded:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(X)\n",
    "\n",
    "print(\"Raw preds:\\n\", y_pred)\n",
    "print(\"Rounded:\\n\", np.round(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-check block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 3.445695470486246e-14\n"
     ]
    }
   ],
   "source": [
    "def grad_check(model: Sequential, X, y_true, eps=1e-5):\n",
    "    loss_fn = MSELoss()\n",
    "    numeric = []\n",
    "    analytic = model.layers[0].dW.copy()  # shape of first W\n",
    "\n",
    "    # numeric\n",
    "    for idx in np.ndindex(analytic.shape):\n",
    "        model.layers[0].W[idx] += eps\n",
    "        plus = loss_fn(y_true, model.forward(X))\n",
    "        model.layers[0].W[idx] -= 2*eps\n",
    "        minus = loss_fn(y_true, model.forward(X))\n",
    "        model.layers[0].W[idx] += eps\n",
    "        \n",
    "        numeric.append((plus - minus)/(2*eps))\n",
    "    \n",
    "    numeric = np.array(numeric).reshape(analytic.shape)\n",
    "\n",
    "    # analytic (already stored by model.train_step)\n",
    "    model.train_step(X, y_true, loss_fn, opt)  # refreshes dW\n",
    "    analytic = model.layers[0].dW\n",
    "\n",
    "    return np.abs(analytic - numeric).max()\n",
    "\n",
    "print('Max diff:', grad_check(model, X, y_true))  # <1e-7 â†’ pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial predictions:\n",
      " [[0.00321279]\n",
      " [0.99170484]\n",
      " [0.99166099]\n",
      " [0.00996228]]\n",
      "Initial loss: 6.197942939023619e-05\n",
      "\n",
      "Layer 0 params shapes: [(2, 4), (1, 4)]\n",
      "Layer grads shapes: [(2, 4), (1, 4)]\n",
      "Gradients dW:\n",
      " [[-1.27764440e-05 -1.75835763e-05]\n",
      " [ 1.85376577e-05  1.13883587e-05]]\n",
      "Gradients db:\n",
      " [[ 3.30822820e-06 -2.33260785e-06  2.19968000e-06  1.27801622e-07]]\n",
      "\n",
      "Layer 1 not trainable\n",
      "\n",
      "Layer 2 params shapes: [(4, 1), (1, 1)]\n",
      "Layer grads shapes: [(4, 1), (1, 1)]\n",
      "Gradients dW:\n",
      " [[-4.77299017e-05]\n",
      " [ 4.77073961e-05]]\n",
      "Gradients db:\n",
      " [[-1.43254514e-05]]\n",
      "\n",
      "Layer 3 not trainable\n",
      "Param change norm: 3.324000912889045e-05\n",
      "Param change norm: 4.608726352107381e-06\n",
      "Param change norm: 7.125137681653467e-05\n",
      "Param change norm: 1.4325451397789735e-05\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "y_pred = model.forward(X)\n",
    "print(\"Initial predictions:\\n\", y_pred)\n",
    "loss = loss_fn(y, y_pred)\n",
    "print(\"Initial loss:\", loss)\n",
    "\n",
    "# Backprop once\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "\n",
    "# Show gradients stored in Dense layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if layer.trainable:\n",
    "        print(f\"\\nLayer {i} params shapes:\", [p.shape for p in [layer.W, layer.b]])\n",
    "        print(\"Layer grads shapes:\", [g.shape for g in [layer.dW, layer.db]])\n",
    "        print(\"Gradients dW:\\n\", layer.dW[:2,:2])\n",
    "        print(\"Gradients db:\\n\", layer.db[:,:])\n",
    "    else:\n",
    "        print(f\"\\nLayer {i} not trainable\")\n",
    "\n",
    "# Copy params\n",
    "old_params = [p.copy() for layer in model.layers if layer.trainable for p in [layer.W, layer.b]]\n",
    "\n",
    "# Step optimizer\n",
    "opt.step(model.layers)\n",
    "\n",
    "# Compare params changed\n",
    "new_params = [p for layer in model.layers if layer.trainable for p in [layer.W, layer.b]]\n",
    "for old, new in zip(old_params, new_params):\n",
    "    print(\"Param change norm:\", np.linalg.norm(new - old))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
