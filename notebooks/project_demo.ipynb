{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE473s Project\n",
    "\n",
    "## Part 1 (XOR)\n",
    "Implement our custom library, test it with [XOR] problem, validate it, and also implement the same exact problem using [TesnorFlow] or [Keras] then compare the results.\n",
    "\n",
    "### Section-1 (Gradient Checking)\n",
    "\n",
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_true = np.array([[0],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build netwrok with the custom library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from lib import Sequential, Dense, Tanh, Sigmoid, MSELoss, SGDOptimizer as SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2, 4, 1.0), Tanh(),\n",
    "    Dense(4, 1, 1.0), Sigmoid()\n",
    "])\n",
    "\n",
    "opt = SGD(learning_rate=1.0)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.12433801468652511\n",
      "Epoch 100, Loss: 0.010525351475778776\n",
      "Epoch 200, Loss: 0.004504717570086526\n",
      "Epoch 300, Loss: 0.002771605387911751\n",
      "Epoch 400, Loss: 0.0019767820050343235\n",
      "Epoch 500, Loss: 0.0015266936821711889\n",
      "Epoch 600, Loss: 0.0012390314484556457\n",
      "Epoch 700, Loss: 0.0010401382532087796\n",
      "Epoch 800, Loss: 0.0008948132132660921\n",
      "Epoch 900, Loss: 0.0007841943890321982\n",
      "Epoch 1000, Loss: 0.0006972967921973738\n",
      "Epoch 1100, Loss: 0.000627303476862028\n",
      "Epoch 1200, Loss: 0.0005697668834102934\n",
      "Epoch 1300, Loss: 0.0005216653400751481\n",
      "Epoch 1400, Loss: 0.0004808760505330671\n",
      "Epoch 1500, Loss: 0.0004458651615112134\n",
      "Epoch 1600, Loss: 0.00041549748069386245\n",
      "Epoch 1700, Loss: 0.00038891531028111817\n",
      "Epoch 1800, Loss: 0.0003654588322447541\n",
      "Epoch 1900, Loss: 0.00034461234984691516\n",
      "Epoch 2000, Loss: 0.000325967109300304\n",
      "Epoch 2100, Loss: 0.00030919503803854915\n",
      "Epoch 2200, Loss: 0.0002940298410829069\n",
      "Epoch 2300, Loss: 0.00028025316186416893\n",
      "Epoch 2400, Loss: 0.00026768429500523597\n",
      "Epoch 2500, Loss: 0.0002561724329746859\n",
      "Epoch 2600, Loss: 0.00024559074844926005\n",
      "Epoch 2700, Loss: 0.0002358318254466278\n",
      "Epoch 2800, Loss: 0.00022680409431851103\n",
      "Epoch 2900, Loss: 0.00021842902280784398\n",
      "Epoch 3000, Loss: 0.00021063888280452374\n",
      "Epoch 3100, Loss: 0.00020337495992316912\n",
      "Epoch 3200, Loss: 0.00019658610691183448\n",
      "Epoch 3300, Loss: 0.00019022756637530864\n",
      "Epoch 3400, Loss: 0.00018426000617487864\n",
      "Epoch 3500, Loss: 0.0001786487240647056\n",
      "Epoch 3600, Loss: 0.00017336298796475713\n",
      "Epoch 3700, Loss: 0.00016837548567407215\n",
      "Epoch 3800, Loss: 0.00016366186344783724\n",
      "Epoch 3900, Loss: 0.00015920033716183265\n",
      "Epoch 4000, Loss: 0.00015497136310382652\n",
      "Epoch 4100, Loss: 0.0001509573580070609\n",
      "Epoch 4200, Loss: 0.00014714245995526052\n",
      "Epoch 4300, Loss: 0.00014351232337425147\n",
      "Epoch 4400, Loss: 0.00014005394258106448\n",
      "Epoch 4500, Loss: 0.0001367554993619827\n",
      "Epoch 4600, Loss: 0.00013360623085249818\n",
      "Epoch 4700, Loss: 0.00013059631463769613\n",
      "Epoch 4800, Loss: 0.00012771676851412955\n",
      "Epoch 4900, Loss: 0.000124959362779304\n",
      "Epoch 5000, Loss: 0.0001223165432621472\n",
      "Epoch 5100, Loss: 0.00011978136359287712\n",
      "Epoch 5200, Loss: 0.0001173474254456565\n",
      "Epoch 5300, Loss: 0.00011500882568177173\n",
      "Epoch 5400, Loss: 0.0001127601094826095\n",
      "Epoch 5500, Loss: 0.00011059622869638371\n",
      "Epoch 5600, Loss: 0.00010851250473522541\n",
      "Epoch 5700, Loss: 0.00010650459545390379\n",
      "Epoch 5800, Loss: 0.00010456846552117114\n",
      "Epoch 5900, Loss: 0.00010270035986206001\n",
      "Epoch 6000, Loss: 0.0001008967798066668\n",
      "Epoch 6100, Loss: 9.915446162942461e-05\n",
      "Epoch 6200, Loss: 9.747035720435632e-05\n",
      "Epoch 6300, Loss: 9.584161653718634e-05\n",
      "Epoch 6400, Loss: 9.426557196553023e-05\n",
      "Epoch 6500, Loss: 9.27397238445021e-05\n",
      "Epoch 6600, Loss: 9.126172755755048e-05\n",
      "Epoch 6700, Loss: 8.982938171176214e-05\n",
      "Epoch 6800, Loss: 8.844061739367038e-05\n",
      "Epoch 6900, Loss: 8.709348837622894e-05\n",
      "Epoch 7000, Loss: 8.57861621802417e-05\n",
      "Epoch 7100, Loss: 8.451691190467718e-05\n",
      "Epoch 7200, Loss: 8.328410874986516e-05\n",
      "Epoch 7300, Loss: 8.208621516608073e-05\n",
      "Epoch 7400, Loss: 8.092177856741465e-05\n",
      "Epoch 7500, Loss: 7.978942555731438e-05\n",
      "Epoch 7600, Loss: 7.868785661793182e-05\n",
      "Epoch 7700, Loss: 7.761584122043612e-05\n",
      "Epoch 7800, Loss: 7.657221331794047e-05\n",
      "Epoch 7900, Loss: 7.555586718658489e-05\n",
      "Epoch 8000, Loss: 7.456575358385244e-05\n",
      "Epoch 8100, Loss: 7.360087619628445e-05\n",
      "Epoch 8200, Loss: 7.266028835149448e-05\n",
      "Epoch 8300, Loss: 7.174308997185872e-05\n",
      "Epoch 8400, Loss: 7.084842474944045e-05\n",
      "Epoch 8500, Loss: 6.997547752363867e-05\n",
      "Epoch 8600, Loss: 6.912347184482112e-05\n",
      "Epoch 8700, Loss: 6.82916677087616e-05\n",
      "Epoch 8800, Loss: 6.747935944805423e-05\n",
      "Epoch 8900, Loss: 6.668587376803425e-05\n",
      "Epoch 9000, Loss: 6.591056791573355e-05\n",
      "Epoch 9100, Loss: 6.515282797154049e-05\n",
      "Epoch 9200, Loss: 6.441206725405479e-05\n",
      "Epoch 9300, Loss: 6.368772482951477e-05\n",
      "Epoch 9400, Loss: 6.297926411787549e-05\n",
      "Epoch 9500, Loss: 6.228617158834156e-05\n",
      "Epoch 9600, Loss: 6.160795553770206e-05\n",
      "Epoch 9700, Loss: 6.094414494543702e-05\n",
      "Epoch 9800, Loss: 6.0294288400014965e-05\n",
      "Epoch 9900, Loss: 5.9657953091286017e-05\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "model.fit(X, y_true, loss_fn, opt, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw preds:\n",
      " [[0.49855366]\n",
      " [0.53412775]\n",
      " [0.55094336]\n",
      " [0.61886783]]\n",
      "Rounded:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(X)\n",
    "\n",
    "print(\"Raw preds:\\n\", y_pred)\n",
    "print(\"Rounded:\\n\", np.round(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-check block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 1.831335037501096e-14\n"
     ]
    }
   ],
   "source": [
    "def grad_check(model: Sequential, X, y_true, eps=1e-5):\n",
    "    loss_fn = MSELoss()\n",
    "    numeric = []\n",
    "    analytic = model.layers[0].dW.copy()  # shape of first W\n",
    "\n",
    "    # numeric\n",
    "    for idx in np.ndindex(analytic.shape):\n",
    "        model.layers[0].W[idx] += eps\n",
    "        plus = loss_fn(y_true, model.forward(X))\n",
    "        model.layers[0].W[idx] -= 2*eps\n",
    "        minus = loss_fn(y_true, model.forward(X))\n",
    "        model.layers[0].W[idx] += eps\n",
    "        \n",
    "        numeric.append((plus - minus)/(2*eps))\n",
    "    \n",
    "    numeric = np.array(numeric).reshape(analytic.shape)\n",
    "\n",
    "    # analytic (already stored by model.train_step)\n",
    "    model.train_step(X, y_true, loss_fn, opt)  # refreshes dW\n",
    "    analytic = model.layers[0].dW\n",
    "\n",
    "    return np.abs(analytic - numeric).max()\n",
    "\n",
    "print('Max diff:', grad_check(model, X, y_true))  # <1e-7 â†’ pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial predictions:\n",
      " [[0.00338321]\n",
      " [0.99211866]\n",
      " [0.99116568]\n",
      " [0.00919011]]\n",
      "Initial loss: 5.901622527026913e-05\n",
      "\n",
      "Layer 0 params shapes: [(2, 4), (1, 4)]\n",
      "Layer grads shapes: [(2, 4), (1, 4)]\n",
      "Gradients dW:\n",
      " [[ 9.26883038e-06 -1.49909905e-05]\n",
      " [ 9.32883393e-06  1.00438532e-05]]\n",
      "Gradients db:\n",
      " [[-2.50199832e-06 -2.47573612e-06  3.19412689e-06 -3.19480289e-06]]\n",
      "\n",
      "Layer 1 not trainable\n",
      "\n",
      "Layer 2 params shapes: [(4, 1), (1, 1)]\n",
      "Layer grads shapes: [(4, 1), (1, 1)]\n",
      "Gradients dW:\n",
      " [[2.65686885e-05]\n",
      " [3.29954470e-05]]\n",
      "Gradients db:\n",
      " [[-2.19461969e-05]]\n",
      "\n",
      "Layer 3 not trainable\n",
      "Param change norm: 3.4864928962574566e-05\n",
      "Param change norm: 5.726995463300427e-06\n",
      "Param change norm: 6.658812622837672e-05\n",
      "Param change norm: 2.1946196918420924e-05\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "y_pred = model.forward(X)\n",
    "print(\"Initial predictions:\\n\", y_pred)\n",
    "loss = loss_fn(y, y_pred)\n",
    "print(\"Initial loss:\", loss)\n",
    "\n",
    "# Backprop once\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "\n",
    "# Show gradients stored in Dense layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if layer.trainable:\n",
    "        print(f\"\\nLayer {i} params shapes:\", [p.shape for p in [layer.W, layer.b]])\n",
    "        print(\"Layer grads shapes:\", [g.shape for g in [layer.dW, layer.db]])\n",
    "        print(\"Gradients dW:\\n\", layer.dW[:2,:2])\n",
    "        print(\"Gradients db:\\n\", layer.db[:,:])\n",
    "    else:\n",
    "        print(f\"\\nLayer {i} not trainable\")\n",
    "\n",
    "# Copy params\n",
    "old_params = [p.copy() for layer in model.layers if layer.trainable for p in [layer.W, layer.b]]\n",
    "\n",
    "# Step optimizer\n",
    "opt.step(model.layers)\n",
    "\n",
    "# Compare params changed\n",
    "new_params = [p for layer in model.layers if layer.trainable for p in [layer.W, layer.b]]\n",
    "for old, new in zip(old_params, new_params):\n",
    "    print(\"Param change norm:\", np.linalg.norm(new - old))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
